{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import hydra\n",
    "\n",
    "from omegaconf import DictConfig\n",
    "from omegaconf.omegaconf import OmegaConf\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series = np.random.rand(1000, 300)\n",
    "time_series[500:560, 100:200] += 0.3\n",
    "time_series = torch.from_numpy(time_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = torch.from_numpy(np.abs(np.indices((100,100))[0] - np.indices((100,100))[1]))\n",
    "sigma = torch.ones(100).view(100, 1) * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = torch.ones(10,10) * torch.arange(10).view(10,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = torch.ones(10,10) * torch.arange(10).view(1,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/spencerbraun/.pyenv/versions/3.8.11/envs/dl/lib/python3.8/site-packages/torch/nn/functional.py:2747: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(3.4057)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda row: F.kl_div(P[row,:], S[row,:]) + F.kl_div(S[row,:], P[row,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [2., 2., 2., 2., 2., 2., 2., 2., 2., 2.],\n",
       "        [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n",
       "        [4., 4., 4., 4., 4., 4., 4., 4., 4., 4.],\n",
       "        [5., 5., 5., 5., 5., 5., 5., 5., 5., 5.],\n",
       "        [6., 6., 6., 6., 6., 6., 6., 6., 6., 6.],\n",
       "        [7., 7., 7., 7., 7., 7., 7., 7., 7., 7.],\n",
       "        [8., 8., 8., 8., 8., 8., 8., 8., 8., 8.],\n",
       "        [9., 9., 9., 9., 9., 9., 9., 9., 9., 9.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.1000, 0.2000, 0.3000, 0.4000, 0.5000, 0.6000, 0.7000, 0.8000,\n",
       "        0.9000, 1.0000, 1.1000, 1.2000, 1.3000, 1.4000, 1.5000, 1.6000, 1.7000,\n",
       "        1.8000, 1.9000, 2.0000, 2.1000, 2.2000, 2.3000, 2.4000, 2.5000, 2.6000,\n",
       "        2.7000, 2.8000, 2.9000, 3.0000, 3.1000, 3.2000, 3.3000, 3.4000, 3.5000,\n",
       "        3.6000, 3.7000, 3.8000, 3.9000, 4.0000, 4.1000, 4.2000, 4.3000, 4.4000,\n",
       "        4.5000, 4.6000, 4.7000, 4.8000, 4.9000, 5.0000, 5.1000, 5.2000, 5.3000,\n",
       "        5.4000, 5.5000, 5.6000, 5.7000, 5.8000, 5.9000, 6.0000, 6.1000, 6.2000,\n",
       "        6.3000, 6.4000, 6.5000, 6.6000, 6.7000, 6.8000, 6.9000, 7.0000, 7.1000,\n",
       "        7.2000, 7.3000, 7.4000, 7.5000, 7.6000, 7.7000, 7.8000, 7.9000, 8.0000,\n",
       "        8.1000, 8.2000, 8.3000, 8.4000, 8.5000, 8.6000, 8.7000, 8.8000, 8.9000,\n",
       "        9.0000, 9.1000, 9.2000, 9.3000, 9.4000, 9.5000, 9.6000, 9.7000, 9.8000,\n",
       "        9.9000])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(0, 10, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sigma' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/8w/r6kg1v9x7bbfzf9dw9gjslc80000gn/T/ipykernel_95641/3924366364.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msigma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sigma' is not defined"
     ]
    }
   ],
   "source": [
    "torch.exp(p.pow(2) / (sigma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'p' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/8w/r6kg1v9x7bbfzf9dw9gjslc80000gn/T/ipykernel_95641/3546984218.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgaussian\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mgaussian\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mgaussian\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'p' is not defined"
     ]
    }
   ],
   "source": [
    "gaussian = torch.normal(p.float(), sigma)\n",
    "gaussian /= gaussian.sum(dim=-1).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gaussian' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/8w/r6kg1v9x7bbfzf9dw9gjslc80000gn/T/ipykernel_95641/494294515.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgaussian\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'gaussian' is not defined"
     ]
    }
   ],
   "source": [
    "gaussian[0,:].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "         0.1000],\n",
       "        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "         0.1000],\n",
       "        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "         0.1000],\n",
       "        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "         0.1000],\n",
       "        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "         0.1000]])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(5,10)/ torch.ones(5,10).sum(dim=-1).view(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10., 10., 10., 10., 10.])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(5,10).sum(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.6336e-02, 7.9632e-01, 1.7809e+00, 3.8046e+00, 2.3193e+00, 4.2680e+00,\n",
       "        8.3718e+00, 7.3727e+00, 6.7060e+00, 1.1051e+01, 1.0537e+01, 1.0064e+01,\n",
       "        1.1236e+01, 1.1610e+01, 1.4704e+01, 1.5483e+01, 1.6960e+01, 1.7324e+01,\n",
       "        1.8780e+01, 1.9655e+01, 1.9641e+01, 2.2539e+01, 2.2296e+01, 2.1461e+01,\n",
       "        2.3893e+01, 2.5538e+01, 2.7476e+01, 2.7799e+01, 2.8607e+01, 3.0070e+01,\n",
       "        2.9921e+01, 3.0479e+01, 3.2850e+01, 3.2870e+01, 3.5183e+01, 3.4523e+01,\n",
       "        3.5027e+01, 3.6092e+01, 3.7668e+01, 3.8510e+01, 4.0233e+01, 4.1136e+01,\n",
       "        4.1330e+01, 4.4310e+01, 4.5134e+01, 4.6767e+01, 4.7380e+01, 4.6682e+01,\n",
       "        4.8102e+01, 4.8557e+01, 5.0157e+01, 5.1855e+01, 5.2537e+01, 5.3827e+01,\n",
       "        5.4526e+01, 5.5917e+01, 5.6168e+01, 5.6827e+01, 5.9424e+01, 5.6755e+01,\n",
       "        5.9634e+01, 6.0564e+01, 5.9774e+01, 6.2367e+01, 6.1566e+01, 6.5457e+01,\n",
       "        6.7245e+01, 6.8038e+01, 6.8918e+01, 7.0426e+01, 7.0787e+01, 7.1730e+01,\n",
       "        7.2374e+01, 7.3203e+01, 7.5236e+01, 7.4870e+01, 7.5451e+01, 7.7490e+01,\n",
       "        7.8921e+01, 7.8873e+01, 7.8570e+01, 8.1434e+01, 8.0754e+01, 8.3289e+01,\n",
       "        8.3406e+01, 8.6066e+01, 8.5195e+01, 8.5312e+01, 8.7704e+01, 8.8209e+01,\n",
       "        9.1053e+01, 9.0458e+01, 9.1672e+01, 9.2758e+01, 9.5681e+01, 9.5560e+01,\n",
       "        9.7419e+01, 9.4693e+01, 9.6847e+01, 9.7406e+01])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.normal(torch.arange(0,100).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnomalyAttention(nn.Module):\n",
    "    def __init__(self, seq_dim, in_channels, out_channels):\n",
    "        super(AnomalyAttention, self).__init__()\n",
    "        self.W = nn.Linear(in_channels, out_channels, bias=False)\n",
    "        self.Q = self.K = self.V = self.sigma = torch.zeros((seq_dim, out_channels))\n",
    "        self.d_model = out_channels\n",
    "        self.n  = seq_dim\n",
    "        self.P = torch.zeros((seq_dim, seq_dim))\n",
    "        self.S = torch.zeros((seq_dim, seq_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        self.initialize(x) # does this make sense?\n",
    "        self.P = self.prior_association()\n",
    "        self.S = self.series_association()\n",
    "        Z = self.reconstruction()\n",
    "\n",
    "        return Z\n",
    "\n",
    "    def initialize(self, x):\n",
    "        # self.d_model = x.shape[-1]\n",
    "        self.Q = self.K = self.V = self.sigma = self.W(x)\n",
    "        \n",
    "\n",
    "    def prior_association(self):\n",
    "        p = torch.from_numpy(\n",
    "            np.abs(\n",
    "                np.indices((self.n,self.n))[0] - \n",
    "                np.indices((self.n,self.n))[1]\n",
    "                )\n",
    "            )\n",
    "        gaussian = torch.normal(p.float(), self.sigma[:,0].abs())\n",
    "        gaussian /= gaussian.sum(dim=-1).view(-1, 1)\n",
    "\n",
    "        return gaussian\n",
    "\n",
    "    def series_association(self):\n",
    "        return F.softmax((self.Q @ self.K.T) / math.sqrt(self.d_model), dim=0)\n",
    "\n",
    "    def reconstruction(self):\n",
    "        return self.S @ self.V\n",
    "\n",
    "    def association_discrepancy(self):\n",
    "        return F.kl_div(self.P, self.S) + F.kl_div(self.S, self.P) #not going to be correct dimensions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnomalyTransformerBlock(nn.Module):\n",
    "    def __init__(self, seq_dim, feat_dim):\n",
    "        super().__init__()\n",
    "        self.seq_dim, self.feat_dim = seq_dim, feat_dim\n",
    "       \n",
    "        self.attention = AnomalyAttention(self.seq_dim, self.feat_dim, self.feat_dim)\n",
    "        self.ln1 = nn.LayerNorm(self.feat_dim)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(self.feat_dim, self.feat_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.ln2 = nn.LayerNorm(self.feat_dim)\n",
    "        self.association_discrepancy = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_identity = x \n",
    "        x = self.attention(x)\n",
    "        z = self.ln1(x + x_identity)\n",
    "        \n",
    "        z_identity = z\n",
    "        z = self.ff(z)\n",
    "        z = self.ln2(z + z_identity)\n",
    "\n",
    "        self.association_discrepancy = self.attention.association_discrepancy().detach()\n",
    "        \n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnomalyTransformer(nn.Module):\n",
    "    def __init__(self, seqs, in_channels, layers, lambda_):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.ModuleList([\n",
    "            AnomalyTransformerBlock(seqs, in_channels) for _ in range(layers)\n",
    "        ])\n",
    "        self.output = None\n",
    "        self.lambda_ = lambda_\n",
    "        self.assoc_discrepancy = torch.zeros((seqs, len(self.blocks)))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for idx, block in enumerate(self.blocks):\n",
    "            x = block(x)\n",
    "            self.assoc_discrepancy[:, idx] = block.association_discrepancy\n",
    "        \n",
    "        self.assoc_discrepancy = self.assoc_discrepancy.sum(dim=1) #N x 1\n",
    "        self.output = x\n",
    "        return x\n",
    "\n",
    "    def loss(self, x):\n",
    "        l2_norm = torch.linalg.matrix_norm(self.output - x, ord=2)\n",
    "        return l2_norm + (self.lambda_ * self.assoc_discrepancy.mean())\n",
    "\n",
    "    def anomaly_score(self, x):\n",
    "        score = F.softmax(-self.assoc_discrepancy, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AnomalyTransformer(seqs=1000, in_channels=300, layers=3, lambda_=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/spencerbraun/.pyenv/versions/3.8.11/envs/dl/lib/python3.8/site-packages/torch/nn/functional.py:2747: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-2.2100,  0.2870, -0.2157,  ..., -1.2898, -1.4303, -0.3504],\n",
       "        [-1.1387, -0.4830, -0.9881,  ..., -1.1120, -2.4669, -0.5230],\n",
       "        [-0.3306, -0.7025, -1.5827,  ..., -0.3628, -1.8427,  0.7217],\n",
       "        ...,\n",
       "        [-0.5868, -0.5519, -1.9108,  ..., -0.4716, -2.8175, -1.0170],\n",
       "        [-1.6752, -0.7690, -2.3892,  ..., -1.6920, -2.6238,  0.9201],\n",
       "        [-1.0995, -0.0956, -0.5864,  ..., -2.5304, -2.2143, -0.5381]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(time_series.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(508.2613, dtype=torch.float64, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.loss(time_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f28f51a40c57d9bec6a3c7c5ba9f41b1bc9273fe115e19bff919e2ad7386eeca"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('dl': pyenv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
